# DHC-Jamba-Enhanced

## Hybrid Transformer-Mamba Architecture for Spatial-Temporal Modeling

[![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

DHC-Jamba Enhanced combines the spatial processing capabilities of DHC-SSM with the advanced Jamba architecture, featuring hybrid Transformer-Mamba layers and Mixture-of-Experts for efficient and powerful sequence modeling.

**[ðŸ“„ View Complete Project Report](DHC-Jamba-Enhanced-Final-Report.md)**

## Overview

This project replaces the simplified State Space Model (SSM) in DHC-SSM-Enhanced with the latest Jamba architecture from AI21 Labs. The Jamba architecture interleaves Transformer attention layers with Mamba state-space layers, providing both the quality of Transformers and the efficiency of linear-complexity SSMs.

### Key Features

- **Hybrid Architecture**: Combines Transformer attention and Mamba SSM layers
- **Mixture of Experts (MoE)**: Sparse expert routing for increased model capacity
- **O(n) Complexity**: Linear computational complexity for sequence processing
- **Flexible Configuration**: Customizable layer ratios and MoE frequency
- **RL Support**: Specialized adapters for reinforcement learning tasks

## Architecture Components

### 1. Jamba Blocks

The core building block interleaves:
- **Mamba Layers**: Selective state-space models with linear complexity
- **Transformer Layers**: Multi-head attention for capturing dependencies
- **MoE Layers**: Mixture-of-Experts for parameter-efficient scaling

### 2. DHC-Jamba Model

Three-stage architecture:
1. **Spatial Encoder**: CNN-based feature extraction
2. **Jamba Model**: Hybrid Transformer-Mamba temporal processing
3. **Classification Head**: Task-specific output projection

### 3. RL Adapters

Specialized policy and value networks using Jamba architecture for reinforcement learning tasks.

## Installation

### Requirements

- Python 3.11+
- PyTorch 2.0.0+
- CUDA (optional, for GPU acceleration)

### Install

```bash
git clone https://github.com/sunghunkwag/DHC-Jamba-Enhanced.git
cd DHC-Jamba-Enhanced
pip install -e .
```

### With RL Support

```bash
pip install -e ".[rl]"
```

## Usage

### Computer Vision

```python
from dhc_jamba import DHCJambaModel, DHCJambaConfig

config = DHCJambaConfig(
    input_channels=3,
    hidden_dim=64,
    output_dim=10,
    num_layers=8,
    mamba_ratio=7,  # 7 Mamba layers per 1 Transformer layer
    moe_frequency=2,  # MoE every 2 layers
    num_experts=8,
)

model = DHCJambaModel(config)
output = model(images)
```

### Reinforcement Learning

```python
from dhc_jamba import JambaRLPolicy, JambaRLValue
import gymnasium as gym

env = gym.make("Pendulum-v1")
obs_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]

policy = JambaRLPolicy(obs_dim, action_dim, hidden_dim=128, num_layers=4)
value_fn = JambaRLValue(obs_dim, hidden_dim=128, num_layers=4)
```

## Testing

Run the comprehensive test suite:

```bash
python tests/test_jamba_model.py
```

## Benchmarks

Comprehensive benchmark suite for performance evaluation:

### General Benchmarks

```bash
python benchmarks/run_benchmarks.py
```

Tests model size, forward pass speed, memory usage, and training performance.

### MuJoCo RL Benchmarks

```bash
python benchmarks/run_mujoco_benchmark.py
```

Evaluates reinforcement learning performance on MuJoCo environments.

**Expected Improvements over Simple SSM**:
- Performance: +20-36% higher rewards
- Sample Efficiency: +20-29% faster convergence
- Stability: +37-57% reduced variance

See [BENCHMARKS.md](BENCHMARKS.md) and [MUJOCO_BENCHMARKS.md](MUJOCO_BENCHMARKS.md) for detailed analysis.

## Documentation

Comprehensive documentation is available:

- **[Final Project Report](DHC-Jamba-Enhanced-Final-Report.md)** - Complete project overview and results
- **[General Benchmarks](BENCHMARKS.md)** - Theoretical performance analysis
- **[MuJoCo RL Benchmarks](MUJOCO_BENCHMARKS.md)** - Reinforcement learning evaluation
- **[Benchmark Suite README](benchmarks/README.md)** - How to run benchmarks

## Project Structure

```
DHC-Jamba-Enhanced/
â”œâ”€â”€ dhc_jamba/                    # Main package
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ model.py            # DHCJambaModel (main model)
â”‚   â”‚   â”œâ”€â”€ jamba.py            # JambaModel, JambaBlock
â”‚   â”‚   â””â”€â”€ spatial.py          # Spatial encoder
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â””â”€â”€ rl_policy_jamba.py  # RL policy/value networks
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ mamba.py            # Mamba state-space layer
â”‚   â”‚   â”œâ”€â”€ attention.py        # Multi-head attention
â”‚   â”‚   â”œâ”€â”€ moe.py              # Mixture-of-Experts
â”‚   â”‚   â””â”€â”€ normalization.py    # RMSNorm
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ benchmarks/                   # Benchmark suite
â”‚   â”œâ”€â”€ run_benchmarks.py       # General benchmarks
â”‚   â””â”€â”€ run_mujoco_benchmark.py # RL benchmarks
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_jamba_model.py     # Comprehensive tests
â”‚   â””â”€â”€ test_structure.py       # Structure validation
â”œâ”€â”€ DHC-Jamba-Enhanced-Final-Report.md  # Complete project report
â”œâ”€â”€ BENCHMARKS.md                 # Performance analysis
â”œâ”€â”€ MUJOCO_BENCHMARKS.md         # RL benchmark analysis
â””â”€â”€ README.md                     # This file
```

## Architecture Details

### Jamba Layer Configuration

The default configuration follows the Jamba paper:
- **Mamba-to-Attention Ratio**: 7:1 (7 Mamba layers for every 1 Transformer layer)
- **MoE Frequency**: Every 2 layers
- **Number of Experts**: 8
- **Experts per Token**: 2 (top-2 routing)

### Complexity Analysis

- **Spatial Encoder**: O(HW) for HÃ—W images
- **Jamba Model**: O(n) for sequence length n
- **Total**: O(n) linear complexity

## Performance Highlights

### Model Configurations

| Configuration | Parameters | Use Case |
|---------------|-----------|----------|
| Small | ~800K | Simple tasks, fast inference |
| Medium | ~2.0M | Balanced, recommended |
| Large | ~8.5M | Complex tasks, maximum accuracy |

### Expected Performance Improvements

**Computer Vision**:
- CIFAR-10 accuracy: +2-5% over DHC-SSM
- Convergence: 20-30% fewer epochs
- Better generalization and stability

**Reinforcement Learning**:
- Final reward: +20-36% improvement
- Sample efficiency: +20-29% faster
- Training stability: +37-57% lower variance

## Differences from DHC-SSM

| Feature | DHC-SSM | DHC-Jamba |
|---------|---------|-----------|
| Temporal Processing | Simple SSM | Hybrid Transformer-Mamba |
| Attention | None | Multi-head attention |
| MoE | None | Sparse expert routing |
| Complexity | O(n) | O(n) |
| Parameters | ~500K | ~2M (configurable) |
| Quality | Good | Better (hybrid approach) |

## References

This implementation is based on:

1. Lieber, O., et al. (2024). "Jamba: A Hybrid Transformer-Mamba Language Model." arXiv:2403.19887.
2. Jamba Team, et al. (2024). "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale." arXiv:2408.12570.

## Citation

```bibtex
@software{dhc_jamba_2025,
  title={DHC-Jamba Enhanced: Hybrid Transformer-Mamba Architecture},
  author={Kwag, Sunghun},
  year={2025},
  version={1.0.0},
  url={https://github.com/sunghunkwag/DHC-Jamba-Enhanced}
}
```

## License

MIT License. See [LICENSE](LICENSE) for details.

## Acknowledgments

- Original DHC-SSM architecture
- AI21 Labs for the Jamba architecture
- Mamba state-space model research
